<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Statistical Thinking</title>
  <meta name="description" content="Introduction to Statistical Thinking">
  <meta name="generator" content="bookdown 0.5.10 and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Statistical Thinking" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="eleuven/statthink" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Statistical Thinking" />
  
  
  

<meta name="author" content="Benjamin Yakir">


<meta name="date" content="2017-11-26">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ChapInference.html">
<link rel="next" href="ChapConfidence.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Statistical Thinking</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="ChapIntroR.html"><a href="ChapIntroR.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="ChapIntroR.html"><a href="ChapIntroR.html#student-learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Student Learning Objectives</a></li>
<li class="chapter" data-level="1.2" data-path="ChapIntroR.html"><a href="ChapIntroR.html#why-learn-statistics"><i class="fa fa-check"></i><b>1.2</b> Why Learn Statistics?</a></li>
<li class="chapter" data-level="1.3" data-path="ChapIntroR.html"><a href="ChapIntroR.html#statistics"><i class="fa fa-check"></i><b>1.3</b> Statistics</a></li>
<li class="chapter" data-level="1.4" data-path="ChapIntroR.html"><a href="ChapIntroR.html#probability"><i class="fa fa-check"></i><b>1.4</b> Probability</a></li>
<li class="chapter" data-level="1.5" data-path="ChapIntroR.html"><a href="ChapIntroR.html#key-terms"><i class="fa fa-check"></i><b>1.5</b> Key Terms</a></li>
<li class="chapter" data-level="1.6" data-path="ChapIntroR.html"><a href="ChapIntroR.html#the-r-programming-environment"><i class="fa fa-check"></i><b>1.6</b> The <code>R</code> Programming Environment</a><ul>
<li class="chapter" data-level="1.6.1" data-path="ChapIntroR.html"><a href="ChapIntroR.html#some-basic-r-commands"><i class="fa fa-check"></i><b>1.6.1</b> Some Basic <code>R</code> Commands</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="ChapIntroR.html"><a href="ChapIntroR.html#exercises"><i class="fa fa-check"></i><b>1.7</b> Exercises</a></li>
<li class="chapter" data-level="1.8" data-path="ChapIntroR.html"><a href="ChapIntroR.html#summary"><i class="fa fa-check"></i><b>1.8</b> Summary</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#discuss-in-the-forum"><i class="fa fa-check"></i>Discuss in the forum</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ChapData.html"><a href="ChapData.html"><i class="fa fa-check"></i><b>2</b> Sampling and Data Structures</a><ul>
<li class="chapter" data-level="2.1" data-path="ChapData.html"><a href="ChapData.html#student-learning-objectives-1"><i class="fa fa-check"></i><b>2.1</b> Student Learning Objectives</a></li>
<li class="chapter" data-level="2.2" data-path="ChapData.html"><a href="ChapData.html#the-sampled-data"><i class="fa fa-check"></i><b>2.2</b> The Sampled Data</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ChapData.html"><a href="ChapData.html#variation-in-data"><i class="fa fa-check"></i><b>2.2.1</b> Variation in Data</a></li>
<li class="chapter" data-level="2.2.2" data-path="ChapData.html"><a href="ChapData.html#variation-in-samples"><i class="fa fa-check"></i><b>2.2.2</b> Variation in Samples</a></li>
<li class="chapter" data-level="2.2.3" data-path="ChapData.html"><a href="ChapData.html#frequency"><i class="fa fa-check"></i><b>2.2.3</b> Frequency</a></li>
<li class="chapter" data-level="2.2.4" data-path="ChapData.html"><a href="ChapData.html#critical-evaluation"><i class="fa fa-check"></i><b>2.2.4</b> Critical Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ChapData.html"><a href="ChapData.html#readingdata"><i class="fa fa-check"></i><b>2.3</b> Reading Data into <code>R</code></a><ul>
<li class="chapter" data-level="2.3.1" data-path="ChapData.html"><a href="ChapData.html#saving-the-file-and-setting-the-working-directory"><i class="fa fa-check"></i><b>2.3.1</b> Saving the File and Setting the Working Directory</a></li>
<li class="chapter" data-level="2.3.2" data-path="ChapData.html"><a href="ChapData.html#Data_3"><i class="fa fa-check"></i><b>2.3.2</b> Reading a CSV File into <code>R</code></a></li>
<li class="chapter" data-level="2.3.3" data-path="ChapData.html"><a href="ChapData.html#data-types"><i class="fa fa-check"></i><b>2.3.3</b> Data Types</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ChapData.html"><a href="ChapData.html#exercises-1"><i class="fa fa-check"></i><b>2.4</b> Exercises</a></li>
<li class="chapter" data-level="2.5" data-path="ChapData.html"><a href="ChapData.html#summary-1"><i class="fa fa-check"></i><b>2.5</b> Summary</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#discuss-in-the-forum"><i class="fa fa-check"></i>Discuss in the forum</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html"><i class="fa fa-check"></i><b>3</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#student-learning-objectives-2"><i class="fa fa-check"></i><b>3.1</b> Student Learning Objectives</a></li>
<li class="chapter" data-level="3.2" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#displaying-data"><i class="fa fa-check"></i><b>3.2</b> Displaying Data</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#histograms"><i class="fa fa-check"></i><b>3.2.1</b> Histograms</a></li>
<li class="chapter" data-level="3.2.2" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#box-plots"><i class="fa fa-check"></i><b>3.2.2</b> Box Plots</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#measures-of-the-center-of-data"><i class="fa fa-check"></i><b>3.3</b> Measures of the Center of Data</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#skewness-the-mean-and-the-median"><i class="fa fa-check"></i><b>3.3.1</b> Skewness, the Mean and the Median</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#measures-of-the-spread-of-data"><i class="fa fa-check"></i><b>3.4</b> Measures of the Spread of Data</a></li>
<li class="chapter" data-level="3.5" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#exercises-2"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
<li class="chapter" data-level="3.6" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#summary-2"><i class="fa fa-check"></i><b>3.6</b> Summary</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#discuss-in-the-forum"><i class="fa fa-check"></i>Discuss in the forum</a></li>
<li class="chapter" data-level="" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#commonly-used-symbols"><i class="fa fa-check"></i>Commonly Used Symbols</a></li>
<li class="chapter" data-level="" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#commonly-used-expressions"><i class="fa fa-check"></i>Commonly Used Expressions</a></li>
<li class="chapter" data-level="" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#formulas"><i class="fa fa-check"></i>Formulas:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ChapProbability.html"><a href="ChapProbability.html"><i class="fa fa-check"></i><b>4</b> Probability</a><ul>
<li class="chapter" data-level="4.1" data-path="ChapProbability.html"><a href="ChapProbability.html#student-learning-objective"><i class="fa fa-check"></i><b>4.1</b> Student Learning Objective</a></li>
<li class="chapter" data-level="4.2" data-path="ChapProbability.html"><a href="ChapProbability.html#different-forms-of-variability"><i class="fa fa-check"></i><b>4.2</b> Different Forms of Variability</a></li>
<li class="chapter" data-level="4.3" data-path="ChapProbability.html"><a href="ChapProbability.html#a-population"><i class="fa fa-check"></i><b>4.3</b> A Population</a></li>
<li class="chapter" data-level="4.4" data-path="ChapProbability.html"><a href="ChapProbability.html#random-variables"><i class="fa fa-check"></i><b>4.4</b> Random Variables</a><ul>
<li class="chapter" data-level="4.4.1" data-path="ChapProbability.html"><a href="ChapProbability.html#sample-space-and-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Sample Space and Distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="ChapProbability.html"><a href="ChapProbability.html#expectation-and-standard-deviation"><i class="fa fa-check"></i><b>4.4.2</b> Expectation and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ChapProbability.html"><a href="ChapProbability.html#probability-and-statistics"><i class="fa fa-check"></i><b>4.5</b> Probability and Statistics</a></li>
<li class="chapter" data-level="4.6" data-path="ChapProbability.html"><a href="ChapProbability.html#exercises-3"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="ChapProbability.html"><a href="ChapProbability.html#summary-3"><i class="fa fa-check"></i><b>4.7</b> Summary</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapProbability.html"><a href="ChapProbability.html#discussion-in-the-forum"><i class="fa fa-check"></i>Discussion in the Forum</a></li>
<li class="chapter" data-level="" data-path="ChapProbability.html"><a href="ChapProbability.html#summary-of-formulas"><i class="fa fa-check"></i>Summary of Formulas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ChapRandomVar.html"><a href="ChapRandomVar.html"><i class="fa fa-check"></i><b>5</b> Random Variables</a><ul>
<li class="chapter" data-level="5.1" data-path="ChapRandomVar.html"><a href="ChapRandomVar.html#student-learning-objective-1"><i class="fa fa-check"></i><b>5.1</b> Student Learning Objective</a></li>
<li class="chapter" data-level="5.2" data-path="ChapRandomVar.html"><a href="ChapRandomVar.html#discrete-random-variables"><i class="fa fa-check"></i><b>5.2</b> Discrete Random Variables</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ChapRandomVar.html"><a href="ChapRandomVar.html#the-binomial-random-variable"><i class="fa fa-check"></i><b>5.2.1</b> The Binomial Random Variable</a></li>
<li class="chapter" data-level="5.2.2" data-path="ChapRandomVar.html"><a href="ChapRandomVar.html#the-poisson-random-variable"><i class="fa fa-check"></i><b>5.2.2</b> The Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ChapRandomVar.html"><a href="ChapRandomVar.html#RandomVar_5"><i class="fa fa-check"></i><b>5.3</b> Continuous Random Variable</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ChapRandomVar.html"><a href="ChapRandomVar.html#the-uniform-random-variable"><i class="fa fa-check"></i><b>5.3.1</b> The Uniform Random Variable</a></li>
<li class="chapter" data-level="5.3.2" data-path="ChapRandomVar.html"><a href="ChapRandomVar.html#the-exponential-random-variable"><i class="fa fa-check"></i><b>5.3.2</b> The Exponential Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ChapRandomVar.html"><a href="ChapRandomVar.html#sec:RVarExercises"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
<li class="chapter" data-level="5.5" data-path="ChapRandomVar.html"><a href="ChapRandomVar.html#summary-4"><i class="fa fa-check"></i><b>5.5</b> Summary</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#discuss-in-the-forum"><i class="fa fa-check"></i>Discuss in the Forum</a></li>
<li class="chapter" data-level="" data-path="ChapProbability.html"><a href="ChapProbability.html#summary-of-formulas"><i class="fa fa-check"></i>Summary of Formulas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ChapNormal.html"><a href="ChapNormal.html"><i class="fa fa-check"></i><b>6</b> The Normal Random Variable</a><ul>
<li class="chapter" data-level="6.1" data-path="ChapNormal.html"><a href="ChapNormal.html#student-learning-objective-2"><i class="fa fa-check"></i><b>6.1</b> Student Learning Objective</a></li>
<li class="chapter" data-level="6.2" data-path="ChapNormal.html"><a href="ChapNormal.html#the-normal-random-variable"><i class="fa fa-check"></i><b>6.2</b> The Normal Random Variable</a><ul>
<li class="chapter" data-level="6.2.1" data-path="ChapNormal.html"><a href="ChapNormal.html#the-normal-distribution"><i class="fa fa-check"></i><b>6.2.1</b> The Normal Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="ChapNormal.html"><a href="ChapNormal.html#the-standard-normal-distribution"><i class="fa fa-check"></i><b>6.2.2</b> The Standard Normal Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="ChapNormal.html"><a href="ChapNormal.html#computing-percentiles"><i class="fa fa-check"></i><b>6.2.3</b> Computing Percentiles</a></li>
<li class="chapter" data-level="6.2.4" data-path="ChapNormal.html"><a href="ChapNormal.html#outliers-and-the-normal-distribution"><i class="fa fa-check"></i><b>6.2.4</b> Outliers and the Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ChapNormal.html"><a href="ChapNormal.html#approximation-of-the-binomial-distribution"><i class="fa fa-check"></i><b>6.3</b> Approximation of the Binomial Distribution</a><ul>
<li class="chapter" data-level="6.3.1" data-path="ChapNormal.html"><a href="ChapNormal.html#approximate-binomial-probabilities-and-percentiles"><i class="fa fa-check"></i><b>6.3.1</b> Approximate Binomial Probabilities and Percentiles</a></li>
<li class="chapter" data-level="6.3.2" data-path="ChapNormal.html"><a href="ChapNormal.html#continuity-corrections"><i class="fa fa-check"></i><b>6.3.2</b> Continuity Corrections</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ChapNormal.html"><a href="ChapNormal.html#Normal4"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
<li class="chapter" data-level="6.5" data-path="ChapNormal.html"><a href="ChapNormal.html#summary-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#discuss-in-the-forum"><i class="fa fa-check"></i>Discuss in the Forum</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ChapSampDist.html"><a href="ChapSampDist.html"><i class="fa fa-check"></i><b>7</b> The Sampling Distribution</a><ul>
<li class="chapter" data-level="7.1" data-path="ChapSampDist.html"><a href="ChapSampDist.html#student-learning-objective-3"><i class="fa fa-check"></i><b>7.1</b> Student Learning Objective</a></li>
<li class="chapter" data-level="7.2" data-path="ChapSampDist.html"><a href="ChapSampDist.html#the-sampling-distribution"><i class="fa fa-check"></i><b>7.2</b> The Sampling Distribution</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ChapSampDist.html"><a href="ChapSampDist.html#a-random-sample"><i class="fa fa-check"></i><b>7.2.1</b> A Random Sample</a></li>
<li class="chapter" data-level="7.2.2" data-path="ChapSampDist.html"><a href="ChapSampDist.html#sampling-from-a-population"><i class="fa fa-check"></i><b>7.2.2</b> Sampling From a Population</a></li>
<li class="chapter" data-level="7.2.3" data-path="ChapSampDist.html"><a href="ChapSampDist.html#subsec:theoreticalmdls"><i class="fa fa-check"></i><b>7.2.3</b> Theoretical Models</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ChapSampDist.html"><a href="ChapSampDist.html#law-of-large-numbers-and-central-limit-theorem"><i class="fa fa-check"></i><b>7.3</b> Law of Large Numbers and Central Limit Theorem</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ChapSampDist.html"><a href="ChapSampDist.html#the-law-of-large-numbers"><i class="fa fa-check"></i><b>7.3.1</b> The Law of Large Numbers</a></li>
<li class="chapter" data-level="7.3.2" data-path="ChapSampDist.html"><a href="ChapSampDist.html#the-central-limit-theorem-clt"><i class="fa fa-check"></i><b>7.3.2</b> The Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="7.3.3" data-path="ChapSampDist.html"><a href="ChapSampDist.html#applying-the-central-limit-theorem"><i class="fa fa-check"></i><b>7.3.3</b> Applying the Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ChapSampDist.html"><a href="ChapSampDist.html#SampDistEx"><i class="fa fa-check"></i><b>7.4</b> Exercises</a></li>
<li class="chapter" data-level="7.5" data-path="ChapSampDist.html"><a href="ChapSampDist.html#summary-6"><i class="fa fa-check"></i><b>7.5</b> Summary</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapProbability.html"><a href="ChapProbability.html#discussion-in-the-forum"><i class="fa fa-check"></i>Discussion in the Forum</a></li>
<li class="chapter" data-level="" data-path="ChapProbability.html"><a href="ChapProbability.html#summary-of-formulas"><i class="fa fa-check"></i>Summary of Formulas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="overview-and-integration.html"><a href="overview-and-integration.html"><i class="fa fa-check"></i><b>8</b> Overview and Integration</a><ul>
<li class="chapter" data-level="8.1" data-path="overview-and-integration.html"><a href="overview-and-integration.html#student-learning-objective-4"><i class="fa fa-check"></i><b>8.1</b> Student Learning Objective</a></li>
<li class="chapter" data-level="8.2" data-path="overview-and-integration.html"><a href="overview-and-integration.html#an-overview"><i class="fa fa-check"></i><b>8.2</b> An Overview</a></li>
<li class="chapter" data-level="8.3" data-path="overview-and-integration.html"><a href="overview-and-integration.html#integrated-applications"><i class="fa fa-check"></i><b>8.3</b> Integrated Applications</a><ul>
<li class="chapter" data-level="8.3.1" data-path="overview-and-integration.html"><a href="overview-and-integration.html#example-1"><i class="fa fa-check"></i><b>8.3.1</b> Example 1</a></li>
<li class="chapter" data-level="" data-path="overview-and-integration.html"><a href="overview-and-integration.html#solution"><i class="fa fa-check"></i>Solution:</a></li>
<li class="chapter" data-level="8.3.2" data-path="overview-and-integration.html"><a href="overview-and-integration.html#example-2"><i class="fa fa-check"></i><b>8.3.2</b> Example 2</a></li>
<li class="chapter" data-level="" data-path="overview-and-integration.html"><a href="overview-and-integration.html#solution-1"><i class="fa fa-check"></i>Solution:</a></li>
<li class="chapter" data-level="8.3.3" data-path="overview-and-integration.html"><a href="overview-and-integration.html#example-3"><i class="fa fa-check"></i><b>8.3.3</b> Example 3</a></li>
<li class="chapter" data-level="" data-path="overview-and-integration.html"><a href="overview-and-integration.html#solution-2"><i class="fa fa-check"></i>Solution:</a></li>
<li class="chapter" data-level="8.3.4" data-path="overview-and-integration.html"><a href="overview-and-integration.html#example-4"><i class="fa fa-check"></i><b>8.3.4</b> Example 4</a></li>
<li class="chapter" data-level="" data-path="overview-and-integration.html"><a href="overview-and-integration.html#solution-3"><i class="fa fa-check"></i>Solution</a></li>
<li class="chapter" data-level="8.3.5" data-path="overview-and-integration.html"><a href="overview-and-integration.html#example-5"><i class="fa fa-check"></i><b>8.3.5</b> Example 5</a></li>
<li class="chapter" data-level="" data-path="overview-and-integration.html"><a href="overview-and-integration.html#solution-4"><i class="fa fa-check"></i>Solution</a></li>
<li class="chapter" data-level="" data-path="ChapProbability.html"><a href="ChapProbability.html#discussion-in-the-forum"><i class="fa fa-check"></i>Discussion in the Forum</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ChapInference.html"><a href="ChapInference.html"><i class="fa fa-check"></i><b>9</b> Introduction to Statistical Inference</a><ul>
<li class="chapter" data-level="9.1" data-path="ChapInference.html"><a href="ChapInference.html#student-learning-objectives-3"><i class="fa fa-check"></i><b>9.1</b> Student Learning Objectives</a></li>
<li class="chapter" data-level="9.2" data-path="ChapInference.html"><a href="ChapInference.html#key-terms-1"><i class="fa fa-check"></i><b>9.2</b> Key Terms</a></li>
<li class="chapter" data-level="9.3" data-path="ChapInference.html"><a href="ChapInference.html#the-cars-data-set"><i class="fa fa-check"></i><b>9.3</b> The Cars Data Set</a></li>
<li class="chapter" data-level="9.4" data-path="ChapSampDist.html"><a href="ChapSampDist.html#the-sampling-distribution"><i class="fa fa-check"></i><b>9.4</b> The Sampling Distribution</a><ul>
<li class="chapter" data-level="9.4.1" data-path="ChapInference.html"><a href="ChapInference.html#statistics-1"><i class="fa fa-check"></i><b>9.4.1</b> Statistics</a></li>
<li class="chapter" data-level="9.4.2" data-path="ChapInference.html"><a href="ChapInference.html#the-sampling-distribution-1"><i class="fa fa-check"></i><b>9.4.2</b> The Sampling Distribution</a></li>
<li class="chapter" data-level="9.4.3" data-path="ChapInference.html"><a href="ChapInference.html#theoretical-distributions-of-observations"><i class="fa fa-check"></i><b>9.4.3</b> Theoretical Distributions of Observations</a></li>
<li class="chapter" data-level="9.4.4" data-path="ChapInference.html"><a href="ChapInference.html#sampling-distribution-of-statistics"><i class="fa fa-check"></i><b>9.4.4</b> Sampling Distribution of Statistics</a></li>
<li class="chapter" data-level="9.4.5" data-path="ChapInference.html"><a href="ChapInference.html#the-normal-approximation"><i class="fa fa-check"></i><b>9.4.5</b> The Normal Approximation</a></li>
<li class="chapter" data-level="9.4.6" data-path="ChapInference.html"><a href="ChapInference.html#simulations"><i class="fa fa-check"></i><b>9.4.6</b> Simulations</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="ChapInference.html"><a href="ChapInference.html#exercises-4"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
<li class="chapter" data-level="9.6" data-path="ChapInference.html"><a href="ChapInference.html#summary-7"><i class="fa fa-check"></i><b>9.6</b> Summary</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#discuss-in-the-forum"><i class="fa fa-check"></i>Discuss in the forum</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ChapEstimation.html"><a href="ChapEstimation.html"><i class="fa fa-check"></i><b>10</b> Point Estimation</a><ul>
<li class="chapter" data-level="10.1" data-path="ChapEstimation.html"><a href="ChapEstimation.html#student-learning-objectives-4"><i class="fa fa-check"></i><b>10.1</b> Student Learning Objectives</a></li>
<li class="chapter" data-level="10.2" data-path="ChapEstimation.html"><a href="ChapEstimation.html#estimating-parameters"><i class="fa fa-check"></i><b>10.2</b> Estimating Parameters</a></li>
<li class="chapter" data-level="10.3" data-path="ChapEstimation.html"><a href="ChapEstimation.html#EstimationExp"><i class="fa fa-check"></i><b>10.3</b> Estimation of the Expectation</a><ul>
<li class="chapter" data-level="10.3.1" data-path="ChapEstimation.html"><a href="ChapEstimation.html#the-accuracy-of-the-sample-average"><i class="fa fa-check"></i><b>10.3.1</b> The Accuracy of the Sample Average</a></li>
<li class="chapter" data-level="10.3.2" data-path="ChapEstimation.html"><a href="ChapEstimation.html#ComparingEstimators"><i class="fa fa-check"></i><b>10.3.2</b> Comparing Estimators</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="ChapEstimation.html"><a href="ChapEstimation.html#estimation-of-the-variance-and-standard-deviation"><i class="fa fa-check"></i><b>10.4</b> Estimation of the Variance and Standard Deviation</a></li>
<li class="chapter" data-level="10.5" data-path="ChapEstimation.html"><a href="ChapEstimation.html#EstimationOtherPars"><i class="fa fa-check"></i><b>10.5</b> Estimation of Other Parameters</a></li>
<li class="chapter" data-level="10.6" data-path="ChapEstimation.html"><a href="ChapEstimation.html#exercises-5"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
<li class="chapter" data-level="10.7" data-path="ChapEstimation.html"><a href="ChapEstimation.html#summary-8"><i class="fa fa-check"></i><b>10.7</b> Summary</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#discuss-in-the-forum"><i class="fa fa-check"></i>Discuss in the forum</a></li>
<li class="chapter" data-level="" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#formulas"><i class="fa fa-check"></i>Formulas:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ChapConfidence.html"><a href="ChapConfidence.html"><i class="fa fa-check"></i><b>11</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="11.1" data-path="ChapConfidence.html"><a href="ChapConfidence.html#student-learning-objectives-5"><i class="fa fa-check"></i><b>11.1</b> Student Learning Objectives</a></li>
<li class="chapter" data-level="11.2" data-path="ChapConfidence.html"><a href="ChapConfidence.html#intervals-for-mean-and-proportion"><i class="fa fa-check"></i><b>11.2</b> Intervals for Mean and Proportion</a><ul>
<li class="chapter" data-level="11.2.1" data-path="ChapConfidence.html"><a href="ChapConfidence.html#ConfidenceExamples"><i class="fa fa-check"></i><b>11.2.1</b> Examples of Confidence Intervals</a></li>
<li class="chapter" data-level="11.2.2" data-path="ChapConfidence.html"><a href="ChapConfidence.html#subsec:CImean"><i class="fa fa-check"></i><b>11.2.2</b> Confidence Intervals for the Mean</a></li>
<li class="chapter" data-level="11.2.3" data-path="ChapConfidence.html"><a href="ChapConfidence.html#subsec:CIfrac"><i class="fa fa-check"></i><b>11.2.3</b> Confidence Intervals for a Proportion</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ChapConfidence.html"><a href="ChapConfidence.html#CInormal"><i class="fa fa-check"></i><b>11.3</b> Intervals for Normal Measurements</a><ul>
<li class="chapter" data-level="11.3.1" data-path="ChapConfidence.html"><a href="ChapConfidence.html#confidence-intervals-for-a-normal-mean"><i class="fa fa-check"></i><b>11.3.1</b> Confidence Intervals for a Normal Mean</a></li>
<li class="chapter" data-level="11.3.2" data-path="ChapConfidence.html"><a href="ChapConfidence.html#confidence-intervals-for-a-normal-variance"><i class="fa fa-check"></i><b>11.3.2</b> Confidence Intervals for a Normal Variance</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="ChapConfidence.html"><a href="ChapConfidence.html#choosing-the-sample-size"><i class="fa fa-check"></i><b>11.4</b> Choosing the Sample Size</a></li>
<li class="chapter" data-level="11.5" data-path="ChapConfidence.html"><a href="ChapConfidence.html#exercises-6"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
<li class="chapter" data-level="11.6" data-path="ChapConfidence.html"><a href="ChapConfidence.html#summary-9"><i class="fa fa-check"></i><b>11.6</b> Summary</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#discuss-in-the-forum"><i class="fa fa-check"></i>Discuss in the forum</a></li>
<li class="chapter" data-level="" data-path="ChapConfidence.html"><a href="ChapConfidence.html#formulas-for-confidence-intervals-95-confidence-level"><i class="fa fa-check"></i>Formulas for Confidence Intervals, 95% Confidence Level:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ChapTesting.html"><a href="ChapTesting.html"><i class="fa fa-check"></i><b>12</b> Testing Hypothesis</a><ul>
<li class="chapter" data-level="12.1" data-path="ChapTesting.html"><a href="ChapTesting.html#student-learning-objectives-6"><i class="fa fa-check"></i><b>12.1</b> Student Learning Objectives</a></li>
<li class="chapter" data-level="12.2" data-path="ChapTesting.html"><a href="ChapTesting.html#the-theory-of-hypothesis-testing"><i class="fa fa-check"></i><b>12.2</b> The Theory of Hypothesis Testing</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ChapTesting.html"><a href="ChapTesting.html#an-example-of-hypothesis-testing"><i class="fa fa-check"></i><b>12.2.1</b> An Example of Hypothesis Testing</a></li>
<li class="chapter" data-level="12.2.2" data-path="ChapTesting.html"><a href="ChapTesting.html#the-structure-of-a-statistical-test-of-hypotheses"><i class="fa fa-check"></i><b>12.2.2</b> The Structure of a Statistical Test of Hypotheses</a></li>
<li class="chapter" data-level="12.2.3" data-path="ChapTesting.html"><a href="ChapTesting.html#error-types-and-error-probabilities"><i class="fa fa-check"></i><b>12.2.3</b> Error Types and Error Probabilities</a></li>
<li class="chapter" data-level="12.2.4" data-path="ChapTesting.html"><a href="ChapTesting.html#p-values"><i class="fa fa-check"></i><b>12.2.4</b> <span class="math inline">\(p\)</span>-Values</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ChapTesting.html"><a href="ChapTesting.html#testing-hypothesis-on-expectation"><i class="fa fa-check"></i><b>12.3</b> Testing Hypothesis on Expectation</a></li>
<li class="chapter" data-level="12.4" data-path="ChapTesting.html"><a href="ChapTesting.html#TestFrac"><i class="fa fa-check"></i><b>12.4</b> Testing Hypothesis on Proportion</a></li>
<li class="chapter" data-level="12.5" data-path="ChapTesting.html"><a href="ChapTesting.html#exercises-7"><i class="fa fa-check"></i><b>12.5</b> Exercises</a></li>
<li class="chapter" data-level="12.6" data-path="ChapTesting.html"><a href="ChapTesting.html#summary-10"><i class="fa fa-check"></i><b>12.6</b> Summary</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#discuss-in-the-forum"><i class="fa fa-check"></i>Discuss in the forum</a></li>
<li class="chapter" data-level="" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#formulas"><i class="fa fa-check"></i>Formulas:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ChapTwoSamp.html"><a href="ChapTwoSamp.html"><i class="fa fa-check"></i><b>13</b> Comparing Two Samples</a><ul>
<li class="chapter" data-level="13.1" data-path="ChapTwoSamp.html"><a href="ChapTwoSamp.html#student-learning-objectives-7"><i class="fa fa-check"></i><b>13.1</b> Student Learning Objectives</a></li>
<li class="chapter" data-level="13.2" data-path="ChapTwoSamp.html"><a href="ChapTwoSamp.html#comparing-two-distributions"><i class="fa fa-check"></i><b>13.2</b> Comparing Two Distributions</a></li>
<li class="chapter" data-level="13.3" data-path="ChapTwoSamp.html"><a href="ChapTwoSamp.html#sec:ComparingMeans"><i class="fa fa-check"></i><b>13.3</b> Comparing the Sample Means</a><ul>
<li class="chapter" data-level="13.3.1" data-path="ChapTwoSamp.html"><a href="ChapTwoSamp.html#an-example-of-a-comparison-of-means"><i class="fa fa-check"></i><b>13.3.1</b> An Example of a Comparison of Means</a></li>
<li class="chapter" data-level="13.3.2" data-path="ChapTwoSamp.html"><a href="ChapTwoSamp.html#confidence-interval-for-the-difference"><i class="fa fa-check"></i><b>13.3.2</b> Confidence Interval for the Difference</a></li>
<li class="chapter" data-level="13.3.3" data-path="ChapTwoSamp.html"><a href="ChapTwoSamp.html#the-t-test-for-two-means"><i class="fa fa-check"></i><b>13.3.3</b> The t-Test for Two Means</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="ChapTwoSamp.html"><a href="ChapTwoSamp.html#comparing-sample-variances"><i class="fa fa-check"></i><b>13.4</b> Comparing Sample Variances</a></li>
<li class="chapter" data-level="13.5" data-path="ChapTwoSamp.html"><a href="ChapTwoSamp.html#exercises-8"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
<li class="chapter" data-level="13.6" data-path="ChapTwoSamp.html"><a href="ChapTwoSamp.html#summary-11"><i class="fa fa-check"></i><b>13.6</b> Summary</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#discuss-in-the-forum"><i class="fa fa-check"></i>Discuss in the forum</a></li>
<li class="chapter" data-level="" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#formulas"><i class="fa fa-check"></i>Formulas:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ChapRegression.html"><a href="ChapRegression.html"><i class="fa fa-check"></i><b>14</b> Linear Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="ChapRegression.html"><a href="ChapRegression.html#student-learning-objectives-8"><i class="fa fa-check"></i><b>14.1</b> Student Learning Objectives</a></li>
<li class="chapter" data-level="14.2" data-path="ChapRegression.html"><a href="ChapRegression.html#points-and-lines"><i class="fa fa-check"></i><b>14.2</b> Points and Lines</a><ul>
<li class="chapter" data-level="14.2.1" data-path="ChapRegression.html"><a href="ChapRegression.html#the-scatter-plot"><i class="fa fa-check"></i><b>14.2.1</b> The Scatter Plot</a></li>
<li class="chapter" data-level="14.2.2" data-path="ChapRegression.html"><a href="ChapRegression.html#linear-equation"><i class="fa fa-check"></i><b>14.2.2</b> Linear Equation</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ChapRegression.html"><a href="ChapRegression.html#linear-regression"><i class="fa fa-check"></i><b>14.3</b> Linear Regression</a><ul>
<li class="chapter" data-level="14.3.1" data-path="ChapRegression.html"><a href="ChapRegression.html#fitting-the-regression-line"><i class="fa fa-check"></i><b>14.3.1</b> Fitting the Regression Line</a></li>
<li class="chapter" data-level="14.3.2" data-path="ChapRegression.html"><a href="ChapRegression.html#subsec:Inference"><i class="fa fa-check"></i><b>14.3.2</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ChapRegression.html"><a href="ChapRegression.html#r-squared-and-the-variance-of-residuals"><i class="fa fa-check"></i><b>14.4</b> R-squared and the Variance of Residuals</a></li>
<li class="chapter" data-level="14.5" data-path="ChapRegression.html"><a href="ChapRegression.html#exercises-9"><i class="fa fa-check"></i><b>14.5</b> Exercises</a></li>
<li class="chapter" data-level="14.6" data-path="ChapRegression.html"><a href="ChapRegression.html#summary-12"><i class="fa fa-check"></i><b>14.6</b> Summary</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#discuss-in-the-forum"><i class="fa fa-check"></i>Discuss in the Forum</a></li>
<li class="chapter" data-level="" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#formulas"><i class="fa fa-check"></i>Formulas:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ChapLogistic.html"><a href="ChapLogistic.html"><i class="fa fa-check"></i><b>15</b> A Bernoulli Response</a><ul>
<li class="chapter" data-level="15.1" data-path="ChapLogistic.html"><a href="ChapLogistic.html#student-learning-objectives-9"><i class="fa fa-check"></i><b>15.1</b> Student Learning Objectives</a></li>
<li class="chapter" data-level="15.2" data-path="ChapLogistic.html"><a href="ChapLogistic.html#comparing-sample-proportions"><i class="fa fa-check"></i><b>15.2</b> Comparing Sample Proportions</a></li>
<li class="chapter" data-level="15.3" data-path="ChapLogistic.html"><a href="ChapLogistic.html#logistic-regression"><i class="fa fa-check"></i><b>15.3</b> Logistic Regression</a></li>
<li class="chapter" data-level="15.4" data-path="ChapLogistic.html"><a href="ChapLogistic.html#exercises-10"><i class="fa fa-check"></i><b>15.4</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="ChapIntroR.html"><a href="ChapIntroR.html#discuss-in-the-forum"><i class="fa fa-check"></i>Discuss in the forum</a></li>
<li class="chapter" data-level="" data-path="ChapDescriptiveStat.html"><a href="ChapDescriptiveStat.html#formulas"><i class="fa fa-check"></i>Formulas:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ChapCaseStudies.html"><a href="ChapCaseStudies.html"><i class="fa fa-check"></i><b>16</b> Case Studies</a><ul>
<li class="chapter" data-level="16.1" data-path="ChapCaseStudies.html"><a href="ChapCaseStudies.html#student-learning-objective-5"><i class="fa fa-check"></i><b>16.1</b> Student Learning Objective</a></li>
<li class="chapter" data-level="16.2" data-path="ChapCaseStudies.html"><a href="ChapCaseStudies.html#a-review"><i class="fa fa-check"></i><b>16.2</b> A Review</a></li>
<li class="chapter" data-level="16.3" data-path="ChapCaseStudies.html"><a href="ChapCaseStudies.html#case-studies"><i class="fa fa-check"></i><b>16.3</b> Case Studies</a><ul>
<li class="chapter" data-level="16.3.1" data-path="ChapCaseStudies.html"><a href="ChapCaseStudies.html#physicians-reactions-to-the-size-of-a-patient"><i class="fa fa-check"></i><b>16.3.1</b> Physiciansâ€™ Reactions to the Size of a Patient</a></li>
<li class="chapter" data-level="16.3.2" data-path="ChapCaseStudies.html"><a href="ChapCaseStudies.html#physical-strength-and-job-performance"><i class="fa fa-check"></i><b>16.3.2</b> Physical Strength and Job Performance</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ChapCaseStudies.html"><a href="ChapCaseStudies.html#summary-13"><i class="fa fa-check"></i><b>16.4</b> Summary</a><ul>
<li class="chapter" data-level="16.4.1" data-path="ChapCaseStudies.html"><a href="ChapCaseStudies.html#concluding-remarks"><i class="fa fa-check"></i><b>16.4.1</b> Concluding Remarks</a></li>
<li class="chapter" data-level="16.4.2" data-path="ChapProbability.html"><a href="ChapProbability.html#discussion-in-the-forum"><i class="fa fa-check"></i><b>16.4.2</b> Discussion in the Forum</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html"><i class="fa fa-check"></i>Exercise Solutions</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-1"><i class="fa fa-check"></i>Chapter 1</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-1.1"><i class="fa fa-check"></i>Exercise 1.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-1.2"><i class="fa fa-check"></i>Exercise 1.2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-2"><i class="fa fa-check"></i>Chapter 2</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-2.1"><i class="fa fa-check"></i>Exercise 2.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-2.2"><i class="fa fa-check"></i>Exercise 2.2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-3"><i class="fa fa-check"></i>Chapter 3</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-3.1"><i class="fa fa-check"></i>Exercise 3.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-3.2"><i class="fa fa-check"></i>Exercise 3.2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-4"><i class="fa fa-check"></i>Chapter 4</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-4.1"><i class="fa fa-check"></i>Exercise 4.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-4.2"><i class="fa fa-check"></i>Exercise 4.2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-5"><i class="fa fa-check"></i>Chapter 5</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-5.1"><i class="fa fa-check"></i>Exercise 5.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-5.2"><i class="fa fa-check"></i>Exercise 5.2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-6"><i class="fa fa-check"></i>Chapter 6</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-6.1"><i class="fa fa-check"></i>Exercise 6.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-6.2"><i class="fa fa-check"></i>Exercise 6.2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-7"><i class="fa fa-check"></i>Chapter 7</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-7.1"><i class="fa fa-check"></i>Exercise 7.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-7.2"><i class="fa fa-check"></i>Exercise 7.2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-9"><i class="fa fa-check"></i>Chapter 9</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-9.1"><i class="fa fa-check"></i>Exercise 9.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-9.2"><i class="fa fa-check"></i>Exercise 9.2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-10"><i class="fa fa-check"></i>Chapter 10</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-10.1"><i class="fa fa-check"></i>Exercise 10.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-10.2"><i class="fa fa-check"></i>Exercise 10.2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-11"><i class="fa fa-check"></i>Chapter 11</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-11.1"><i class="fa fa-check"></i>Exercise 11.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-11.2"><i class="fa fa-check"></i>Exercise 11.2</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-11.3"><i class="fa fa-check"></i>Exercise 11.3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-12"><i class="fa fa-check"></i>Chapter 12</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-12.1"><i class="fa fa-check"></i>Exercise 12.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-12.2"><i class="fa fa-check"></i>Exercise 12.2</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-12.3"><i class="fa fa-check"></i>Exercise 12.3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-13"><i class="fa fa-check"></i>Chapter 13</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-13.1"><i class="fa fa-check"></i>Exercise 13.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-13.2"><i class="fa fa-check"></i>Exercise 13.2</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-13.3"><i class="fa fa-check"></i>Exercise 13.3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-14"><i class="fa fa-check"></i>Chapter 14</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-14.1"><i class="fa fa-check"></i>Exercise 14.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-14.2"><i class="fa fa-check"></i>Exercise 14.2</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-14.3"><i class="fa fa-check"></i>Exercise 14.3</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-14.4"><i class="fa fa-check"></i>Exercise 14.4</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-14.5"><i class="fa fa-check"></i>Exercise 14.5</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-14.6"><i class="fa fa-check"></i>Exercise 14.6</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-15"><i class="fa fa-check"></i>Chapter 15</a><ul>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-15.1"><i class="fa fa-check"></i>Exercise 15.1</a></li>
<li class="chapter" data-level="" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-15.2"><i class="fa fa-check"></i>Exercise 15.2</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Thinking</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ChapEstimation" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Point Estimation</h1>
<div id="student-learning-objectives-4" class="section level2">
<h2><span class="header-section-number">10.1</span> Student Learning Objectives</h2>
<p>The subject of this chapter is the estimation of the value of a parameter on the basis of data. An estimator is a statistic that is used for estimation. Criteria for selecting among estimators are discussed, with the goal of seeking an estimator that tends to obtain values that are as close as possible to the value of the parameter. Different examples of estimation problems are presented and analyzed. By the end of this chapter, the student should be able to:</p>
<ul>
<li><p>Recognize issues associated with the estimation of parameters.</p></li>
<li><p>Define the notions of bias, variance and mean squared error (MSE) of an estimator.</p></li>
<li><p>Estimate parameters from data and assess the performance of the estimation procedure.</p></li>
</ul>
</div>
<div id="estimating-parameters" class="section level2">
<h2><span class="header-section-number">10.2</span> Estimating Parameters</h2>
<p>Statistic is the science of data analysis. The primary goal in statistic is to draw meaningful and solid conclusions on a given phenomena on the basis of observed data. Typically, the data emerges as a sample of observations. An observation is the outcome of a measurement (or several measurements) that is taken for a subject that belongs to the sample. These observations may be used in order to investigate the phenomena of interest. The conclusions are drawn from the analysis of the observations.</p>
<p>A key aspect in statistical inference is the association of a probabilistic model to the observations. The basic assumption is that the observed data emerges from some distribution. Usually, the assumption is that the distribution is linked to a theoretical model, such as the Normal, Exponential, Poisson, or any other model that fits the specifications of the measurement taken.</p>
<p>A standard setting in statistical inference is the presence of a sequence of observations. It is presumed that all the observations emerged from a common distribution. The parameters one seeks to estimate are summaries or characteristics of that distribution.</p>
<p>For example, one may be interested in the distribution of price of cars. A reasonable assumption is that the distribution of the prices is the <span class="math inline">\(\mathrm{Exponential}(\lambda)\)</span> distribution. Given an observed sample of prices one may be able to estimate the rate <span class="math inline">\(\lambda\)</span> that specifies the distribution.</p>
<p>The target in statistical point estimation of a parameter is to produce the best possible guess of the value of a parameter on the basis of the available data. The statistic that tries to guess the value of the parameter is called an <em>estimator</em>. The estimator is a formula applied to the data that produces a number. This number is the <em>estimate</em> of the value of the parameter.</p>
<p>An important characteristic of a distribution, which is always of interest, is the expectation of the measurement, namely the central location of the distribution. A natural estimator of the expectation is the sample average. However, one may propose other estimators that make sense, such as the sample mid-range that was presented in the previous chapter. The main topic of this chapter is the identification of criteria that may help us choose which estimator to use for the estimation of which parameter.</p>
<p>In the next section we discuss issues associated with the estimation of the expectation of a measurement. The following section deals with the estimation of the variance and standard deviation â€“ summaries that characterize the spread of the distribution. The last section deals with the theoretical models of distribution that were introduced in the first part of the book. It discusses ways by which one may estimate the parameters that characterize these distributions.</p>
</div>
<div id="EstimationExp" class="section level2">
<h2><span class="header-section-number">10.3</span> Estimation of the Expectation</h2>
<p>A natural candidate for the estimation of the expectation of a random variable on the basis of a sample of observations is the sample average. Consider, as an example, the estimation of the expected price of a car using the information in the data file â€œ<code>cars.csv</code>â€. Let us read the data into a data frame named â€œ<code>cars</code>â€ and compute the average of the variable â€œ<code>price</code>â€:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cars &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;_data/cars.csv&quot;</span>)
<span class="kw">mean</span>(cars<span class="op">$</span>price)</code></pre></div>
<pre><code>## [1] NA</code></pre>
<p>The application of the function â€œ<code>mean</code>â€ for the computation of the sample average produced a missing value. The reason is that the variable â€œ<code>price</code>â€ contains 4 missing values. As default, when applied to a sequence that contains missing values, the function â€œ<code>mean</code>â€ produce as output a missing value.</p>
<p>The behavior of the function â€œ<code>mean</code>â€ at the presence of missing values is determined by the argument â€œ<code>na.rm</code>â€<a href="#fn34" class="footnoteRef" id="fnref34"><sup>34</sup></a>. If we want to compute the average of the non-missing values in the sequence we should specify the argument â€œ<code>na.rm</code>â€ as â€œ<code>TRUE</code>â€. This can be achieved by the inclusion of the expression â€œ<code>na.rm=TRUE</code>â€ in the arguments of the function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(cars<span class="op">$</span>price,<span class="dt">na.rm=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 13207.129</code></pre>
<p>The resulting average price is, approximately, $13,000.</p>
<div id="the-accuracy-of-the-sample-average" class="section level3">
<h3><span class="header-section-number">10.3.1</span> The Accuracy of the Sample Average</h3>
<p>How close is the estimated value of the expectation â€“ the average price â€“ to the expected price?</p>
<p>There is no way of answering this question on the basis of the data we observed. Indeed, we think of the price of a random car as a random variable. The expectation we seek to estimate is the expectation of that random variable. However, the actual value of that expectation is unknown. Hence, not knowing what is the target value, how can we determine the distance between the computed average 13207.13 and that unknown value?</p>
<p>As a remedy for not being able to answer the question we would like to address we, instead, change the question. In the new formulation of the question we ignore the data at hand altogether. The new formulation considers the sample average as a statistic and the question is formulated in terms of the sampling distribution of that statistic. The question, in its new formulation is: How close is the sample average of the price, taken as a random variable, to the expected price?</p>
<p>Notice that in the new formulation of the question the observed average price <span class="math inline">\(\bar x = 13207.13\)</span> has no special role. The question is formulated in terms of the sampling distribution of the sample average (<span class="math inline">\(\bar X\)</span>). The observed average value is only one among many in the sampling distribution of the average.</p>
<p>The advantage of the new formulation of the question is that it can be addressed. We do have means for investigating the closeness of the estimator to the parameter and thereby producing meaningful answers. Specifically, consider the current case where the estimator is the sample average <span class="math inline">\(\bar X\)</span>. This estimator attempts to estimate the expectation <span class="math inline">\(\Expec(X)\)</span> of the measurement, which is the parameter. Assessing the closeness of the estimator to the parameter corresponds to the comparison between the distribution of the random variable, i.e.Â the estimator, and the value of the parameter.</p>
<p>For this comparison we may note that the expectation <span class="math inline">\(\Expec(X)\)</span> is also the expectation of the sample average <span class="math inline">\(\bar X\)</span>. Consequently, in this problem the assessment of the closeness of the estimator to the parameter is equivalent to the investigation of the spread of the distribution of the sample average about its expectation.</p>
<p>Consider an example of such investigation. Imagine that the expected price of a car is equal to $13,000. A question one may ask is how likely it is that the estimatorâ€™s guess at the value is within $1,000 of the actual value? In other words, what is the probability that sample average falls in the range <span class="math inline">\([12,000, 14,000]\)</span>?</p>
<p>Let us investigate this question using simulations. Recall our assumption that the distribution of the price is Exponential. An expectation of 13,000 corresponds to a rate parameter of <span class="math inline">\(\lambda = 1/13,000\)</span>. We simulate the sampling distribution of the estimator by the generation of a sample of 201 Exponential random variables with this rate. The sample average is computed for each sample and stored. The sampling distribution of the sample average is approximated via the production of a large number of sample averages:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lam &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">13000</span>
X.bar &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">10</span><span class="op">^</span><span class="dv">5</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">^</span><span class="dv">5</span>) {
  X &lt;-<span class="st"> </span><span class="kw">rexp</span>(<span class="dv">201</span>,lam)
  X.bar[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(X)
}
<span class="kw">mean</span>(<span class="kw">abs</span>(X.bar <span class="op">-</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>lam) <span class="op">&lt;=</span><span class="st"> </span><span class="dv">1000</span>)</code></pre></div>
<pre><code>## [1] 0.72674</code></pre>
<p>In the last line of the code we compute the probability of being within $1,000 of the expected price. Recall that the expected price in the Exponential case is the reciprocal of the rate <span class="math inline">\(\lambda\)</span>. In this simulation we obtained 0.7247 as an approximation of the probability.</p>
<p>In the case of the sample average we may also apply the Normal approximation in order to assess the probability under consideration. In particular, if <span class="math inline">\(\lambda = 1/13,000\)</span> then the expectation of an Exponential observation is <span class="math inline">\(\Expec(X) = 1/\lambda = 13,000\)</span> and the variance is <span class="math inline">\(\Var(X) = 1/\lambda^2 = (13,000)^2\)</span>. The expectation of the sample average is equal to the expectation of the measurement, 13,000 in this example. The variance of the sample average is equal to the variance of the observation, divided by the sample size. In the current setting it is equal to <span class="math inline">\((13,000)^2/201\)</span>. The standard deviation is equal to the square root of the variance.</p>
<p>The Normal approximation uses the Normal distribution in order to compute probabilities associated with the sample average. The Normal distribution that is used has the same expectation and standard deviation as the sample average:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span><span class="dv">13000</span>
sig &lt;-<span class="st"> </span><span class="dv">13000</span><span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">201</span>)
<span class="kw">pnorm</span>(<span class="dv">14000</span>,mu,sig) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dv">12000</span>,mu,sig)</code></pre></div>
<pre><code>## [1] 0.72453911</code></pre>
<p>The probability of falling within the interval <span class="math inline">\([12000, 14000]\)</span> is computed as the difference between the cumulative Normal probability at 14,000 and the cumulative Normal probability at 12,000.</p>
<p>These cumulative probabilities are computed with the function â€œ<code>pnorm</code>â€. Recall that this function computes the cumulative probability for the Normal distribution. If the first argument is 14,000 then the function produces the probability that a Normal random variable is less than or equal to 14,000. Likewise, if the first argument is 12,000 then the computed probability is the probability of being less than or equal to 12,000. The expectation of the Normal distribution enters in the second argument of the function and the standard deviation enters in the third argument.</p>
<p>The Normal approximation of falling in the interval <span class="math inline">\([12000, 14000]\)</span>, computed as the difference between the two cumulative probabilities, produces 0.7245391 as the probability<a href="#fn35" class="footnoteRef" id="fnref35"><sup>35</sup></a>. Notice that the probability 0.7247 computed in the simulations is in agreement with the Normal approximation.</p>
<p>If we wish to assess the accuracy of the estimator at other values of the parameter, say <span class="math inline">\(\Expec(X) = 12,000\)</span> (which corresponds to <span class="math inline">\(\lambda = 1/12,000\)</span>) or <span class="math inline">\(\Expec(X) = 14,033\)</span>, (which corresponds to <span class="math inline">\(\lambda = 1/14,033\)</span>) all we need to do is change the expression â€œ<code>lam &lt;- 1/13000</code>â€ to the new value and rerun the simulation.</p>
<p>Alternatively, we may use a Normal approximation with modified interval, expectation, and standard deviation. For example, consider the case where the expected price is equal to $12,000. In order to asses the probability that the sample average falls within $1,000 of the parameter we should compute the probability of the interval <span class="math inline">\([11,000, 13,000]\)</span> and change the entries to the first argument of the function â€œ<code>pnorm</code>â€ accordingly. The new expectation is 12,000 and the new standard deviation is <span class="math inline">\(12,000/\sqrt{201}\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span><span class="dv">12000</span>
sig &lt;-<span class="st"> </span><span class="dv">12000</span><span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">201</span>)
<span class="kw">pnorm</span>(<span class="dv">13000</span>,mu,sig) <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dv">11000</span>,mu,sig)</code></pre></div>
<pre><code>## [1] 0.76257755</code></pre>
<p>This time we get that the probability is, approximately, 0.763.</p>
<p>The fact that the computed value of the average 13,207.13 belongs to the interval <span class="math inline">\([12,000, 14,000]\)</span> that was considered in the first analysis but does not belong to the interval <span class="math inline">\([11,000, 13,000]\)</span> that was considered in the second analysis is irrelevant to the conclusions drawn from the analysis. In both cases the theoretical properties of the sample average as an estimator were considered and not its value at specific data.</p>
<p>In the simulation and in the Normal approximation we applied one method for assessing the closeness of the sample average to the expectation it estimates. This method involved the computation of the probability of being within $1,000 of the expected price. The higher this probability, the more accurate is the estimator.</p>
<p>An alternative method for assessing the accuracy of an estimator of the expectation may involve the use of an overall summary of the spread of the distribution. A standard method for quantifying the spread of a distribution about the expectation is the variance (or its square root, the standard deviation). Given an estimator of the expectation of a measurement, the sample average for example, we may evaluate the accuracy of the estimator by considering its variance. The smaller the variance the more accurate is the estimator.</p>
<p>Consider again the case where the sample average is used in order to estimate the expectation of a measurement. In such a situation the variance of the estimator, i.e.Â the variance of the sample average, is obtained as the ratio between the variance of the measurement <span class="math inline">\(\Var(X)\)</span>, divided by the sample size <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[\Var(\bar X) = \Var(X)/n\;.\]</span> Notice that for larger sample sizes the estimator is more accurate. The lager the sample size <span class="math inline">\(n\)</span> the smaller is the variance of the estimator, in which case the values of the estimator tend to be more concentrated about the expectation. Hence, one may make the estimator more accurate by increasing the sample size.</p>
<p>Another method for improving the accuracy of the average of measurements in estimating the expectation is the application of a more accurate measurement device. If the variance <span class="math inline">\(\Var(X)\)</span> of the measurement device decreases so does the variance of the sample average of such measurements.</p>
<p>In the sequel, when we investigate the accuracy of estimators, we will generally use overall summaries of the spread of their distribution around the target value of the parameter.</p>
</div>
<div id="ComparingEstimators" class="section level3">
<h3><span class="header-section-number">10.3.2</span> Comparing Estimators</h3>
<p>Notice that the formulation of the accuracy of estimation that we use replaces the question: â€œHow close is the given value of the estimator to the unknown value of the parameter?â€ by the question: â€œHow close are the unknown (and random) values of the estimator to a given value of the parameter?â€ In the second formulation the question is completely academic and unrelated to actual measurement values. In this academic context we can consider different potential values of the parameter. Once the value of the parameter has been selected it can be treated as known in the context of the academic discussion. Clearly, this does not imply that we actually know what is the true value of the parameter.</p>
<p>The sample average is a natural estimator of the expectation of the measurement. However, one may propose other estimators. For example, when the distribution of the measurement is symmetric about the expectation then the median of the distribution is equal to the expectation. The sample median, which is a natural estimator of the measurement median, is an alternative estimator of the expectation in such case. Which of the two alternatives, the sample average or the sample median, should we prefer as an estimator of the expectation in the case of a symmetric distribution?</p>
<p>The straightforward answer to this question is to prefer the better one, the one which is more accurate. As part of the solved exercises you are asked to compare the sample average to the sample median as estimators of the expectation. Here we compare the sample average to yet another alternative estimator â€“ the mid-range estimator â€“ which is the average between the smallest and the largest observations.</p>
<p>In the comparison between estimators we do not evaluate them in the context of the observed data. Rather, we compare them as random variables. The comparison deals with the properties of the estimators in a given theoretical context. This theoretical context is motivated by the realities of the situation as we know them. But, still, the frame of reference is the theoretical model and not the collected data.</p>
<p>Hence, depending on the context, we may assume in the comparison that the observations emerge from some distribution. We may specify parameter values for this distribution and select the appropriate sample size. After setting the stage we can compare the accuracy of one estimator against that of the other. Assessment at other parameter values in the context of the given theoretical model, or of other theoretical models, may provide insight and enhance our understanding regarding the relative merits and weaknesses of each estimator.</p>
<p>Let us compare the sample average to the sample mid-range as estimators of the expectation in a situation that we design. Consider a Normal measurement <span class="math inline">\(X\)</span> with expectation <span class="math inline">\(\Expec(X) = 3\)</span> and variance that is equal to 2. Assume that the sample size is <span class="math inline">\(n = 100\)</span>. Both estimators, due to the symmetry of the Normal distribution, are centered at the expectation. Hence, we may evaluate the accuracy of the two estimators using their variances. These variances are the measure of the spread of the distributions of each estimator about the target parameter value.</p>
<p>We produce the sampling distribution and compute the variances using a simulation. Recall that the distribution of the mid-range statistic was simulated in the previous chapter. In the computation of the mid-range statistic we used the function â€œ<code>max</code>â€ that computes the maximum value of its input and the function â€œ<code>min</code>â€ that computes the minimum value:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span><span class="dv">3</span>
sig &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span>)
X.bar &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">10</span><span class="op">^</span><span class="dv">5</span>)
mid.range &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">10</span><span class="op">^</span><span class="dv">5</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">^</span><span class="dv">5</span>) {
  X &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>,mu,sig)
  X.bar[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(X)
  mid.range[i] &lt;-<span class="st"> </span>(<span class="kw">max</span>(X)<span class="op">+</span><span class="kw">min</span>(X))<span class="op">/</span><span class="dv">2</span>
}
<span class="kw">var</span>(X.bar)</code></pre></div>
<pre><code>## [1] 0.01989991</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(mid.range)</code></pre></div>
<pre><code>## [1] 0.1863586</code></pre>
<p>We get that the variance of the sample average<a href="#fn36" class="footnoteRef" id="fnref36"><sup>36</sup></a> is approximately equal to 0.02. The variance of the mid-range statistic is approximately equal to 0.185, more than 9 times as large. We see that the accuracy of the sample average is better in this case than the accuracy of the mid-range estimator. Evaluating the two estimators at other values of the parameter will produce the same relation. Hence, in the current example it seems as if the sample average is the better of the two.</p>
<p>Is the sample average necessarily the best estimator for the expectation? The next example will demonstrate that this need not always be the case.</p>
<p>Consider again a situation of observing a sample of size <span class="math inline">\(n=100\)</span>. However, this time the measurement <span class="math inline">\(X\)</span> is Uniform and not Normal. Say <span class="math inline">\(X \sim \mathrm{Uniform}(0.5,5.5)\)</span> has the Uniform distribution over the interval <span class="math inline">\([0.5, 5.5]\)</span>. The expectation of the measurement is equal to 3 like before, since <span class="math inline">\(\Expec(X) = (0.5+5.5)/2 = 3\)</span>. The variance on an observation is <span class="math inline">\(\Var(X) = (5.5 - 0.5)^2/12 = 2.083333\)</span>, not much different from the variance that was used in the Normal case. The Uniform distribution, like the Normal distribution, is a symmetric distribution about the center of the distribution. Hence, using the mid-range statistic as an estimator of the expectation makes sense<a href="#fn37" class="footnoteRef" id="fnref37"><sup>37</sup></a>.</p>
<p>We re-run the simulations, using the function â€œ<code>runif</code>â€ for the simulation of a sample from the Uniform distribution and the parameters of the Uniform distribution instead of the function â€œ<code>rnorm</code>â€ that was used before:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a &lt;-<span class="st"> </span><span class="fl">0.5</span>
b &lt;-<span class="st"> </span><span class="fl">5.5</span>
X.bar &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">10</span><span class="op">^</span><span class="dv">5</span>)
mid.range &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">10</span><span class="op">^</span><span class="dv">5</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">^</span><span class="dv">5</span>) {
  X &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">100</span>,a,b)
  X.bar[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(X)
  mid.range[i] &lt;-<span class="st"> </span>(<span class="kw">max</span>(X)<span class="op">+</span><span class="kw">min</span>(X))<span class="op">/</span><span class="dv">2</span>
}
<span class="kw">var</span>(X.bar)</code></pre></div>
<pre><code>## [1] 0.020657925</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(mid.range)</code></pre></div>
<pre><code>## [1] 0.0012143288</code></pre>
<p>Again, we get that the variance of the sample average is approximately equal to 0.02, which is close to the theoretical value<a href="#fn38" class="footnoteRef" id="fnref38"><sup>38</sup></a>. The variance of mid-range statistic is approximately equal to 0.0012.</p>
<p>Observe that in the current comparison between the sample average and the mid-range estimator we get that the latter is a clear winner. Examination of other values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> for the Uniform distribution will produce the same relation between the two competitors. Hence, we may conclude that for the case of the Uniform distribution the sample average is an inferior estimator.</p>
<p>The last example may serve as yet another reminder that life is never simple. A method that is good in one situation may not be as good in a different situation.</p>
<p>Still, the estimator of choice of the expectation is the sample average. Indeed, in some cases we may find that other methods may produce more accurate estimates. However, in most settings the sample average beats its competitors. The sample average also possesses other useful benefits. Its sampling distribution is always centered at the expectation it is trying to estimate. Its variance has a simple form, i.e.Â it is equal to the variance of the measurement divided by the sample size. Moreover, its sampling distribution can be approximated by the Normal distribution. Henceforth, due to these properties, we will use the sample average whenever estimation of the expectation is required.</p>
</div>
</div>
<div id="estimation-of-the-variance-and-standard-deviation" class="section level2">
<h2><span class="header-section-number">10.4</span> Estimation of the Variance and Standard Deviation</h2>
<p>The spread of the measurement about its expected value may be measured by the variance or by the standard deviation, which is the square root of the variance. The standard estimator for the variance of the measurement is the sample variance and the square root of the sample variance is the default estimator of the standard deviation.</p>
<p>The computation of the sample variance from the data is discussed in ChapterÂ <a href="ChapDescriptiveStat.html#ChapDescriptiveStat">3</a>. Recall that the sample variance is computed via the formula:</p>
<p><span class="math display">\[s^2 = \frac{\mbox{Sum of the squares of the deviations}}{\mbox{Number of values in the sample}-1}= \frac{\sum_{i=1}^n (x_i - \bar x)^2}{n-1}\;,\]</span> where <span class="math inline">\(\bar x\)</span> is the sample average and <span class="math inline">\(n\)</span> is the sample size. The term <span class="math inline">\(x_i-\bar x\)</span> is the deviation from the sample average of the <span class="math inline">\(i\)</span>th observation and <span class="math inline">\(\sum_{i=1}^n (x_i - \bar x)^2\)</span> is the sum of the squares of deviations. It is pointed out in ChapterÂ <a href="ChapDescriptiveStat.html#ChapDescriptiveStat">3</a> that the reason for dividing the sum of squares by <span class="math inline">\((n-1)\)</span>, rather than <span class="math inline">\(n\)</span>, stems from considerations of statistical inference. A promise was made that these reasonings will be discussed in due course. Now we want to deliver on this promise.</p>
<p>Let us compare between two competing estimators for the variance, both considered as random variables. One is the estimator <span class="math inline">\(S^2\)</span>, which is equal to the formula for the sample variance applied to a random sample:</p>
<p><span class="math display">\[S^2 = \frac{\mbox{Sum of the squares of the deviations}}{\mbox{Number of values in the sample}-1}= \frac{\sum_{i=1}^n (X_i - \bar X)^2}{n-1}\;,\]</span> The computation of this statistic can be carried out with the function â€œ<code>var</code>â€.</p>
<p>The second estimator is the one obtained when the sum of squares is divided by the sample size (instead of the sample size minus 1):</p>
<p><span class="math display">\[\frac{\mbox{Sum of the squares of the deviations}}{\mbox{Number of values in the sample}}=\frac{\sum_{i=1}^n (X_i - \bar X)^2}{n}\;.\]</span> Observe that the second estimator can be represented in the form:</p>
<p><span class="math display">\[\frac{\sum_{i=1}^n (X_i - \bar X)^2}{n} =\frac{n-1}{n} \cdot \frac{\sum_{i=1}^n (X_i - \bar X)^2}{n-1}= [(n-1)/n] S^2\;.\]</span> Hence, the second estimator may be obtained by the multiplication of the first estimator <span class="math inline">\(S^2\)</span> by the ratio <span class="math inline">\((n-1)/n\)</span>. We seek to compare between <span class="math inline">\(S^2\)</span> and <span class="math inline">\([(n-1)/n] S^2\)</span> as estimators of the variance.</p>
<p>In order to make the comparison concrete, let us consider it in the context of a Normal measurement with expectation <span class="math inline">\(\mu = 5\)</span> and variance <span class="math inline">\(\sigma^2 = 3\)</span>. Let us assume that the sample is of size 20 (<span class="math inline">\(n=20\)</span>).</p>
<p>Under these conditions we carry out a simulation. Each iteration of the simulation involves the generation of a sample of size <span class="math inline">\(n=20\)</span> from the given Normal distribution. The sample variance <span class="math inline">\(S^2\)</span> is computed from the sample with the application of the function â€œ<code>var</code>â€. The resulting estimate of the variance is stored in an object that is called â€œ<code>X.var</code>â€:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span><span class="dv">5</span>
std &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">3</span>)
X.var &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">10</span><span class="op">^</span><span class="dv">5</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span><span class="op">^</span><span class="dv">5</span>) {
  X &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">20</span>,mu,std)
  X.var[i] &lt;-<span class="st"> </span><span class="kw">var</span>(X)
}</code></pre></div>
<p>The content of the object â€œ<code>X.var</code>â€, at the end of the simulation, approximates the sampling distribution of the estimator <span class="math inline">\(S^2\)</span>.</p>
<p>Our goal is to compare between the performance of the estimator of the variance <span class="math inline">\(S^2\)</span> and that of the alternative estimator. In this alternative estimator the sum of squared deviations is divided by the sample size (<span class="math inline">\(n=20\)</span>) and not by the sample size minus 1 (<span class="math inline">\(n-1 = 19\)</span>). Consequently, the alternative estimator is obtained by multiplying <span class="math inline">\(S^2\)</span> by the ratio <span class="math inline">\(19/20\)</span>. The sampling distribution of the values of <span class="math inline">\(S^2\)</span> is approximated by the content of the object â€œ<code>X.var</code>â€. It follows that the sampling distribution of the alternative estimator is approximated by the object â€œ<code>(19/20)*X.var</code>â€, in which each value of <span class="math inline">\(S^2\)</span> is multiplied by the appropriate ratio. The comparison between the sampling distribution of <span class="math inline">\(S^2\)</span> and the sampling distribution of the alternative estimator is obtained by comparing between â€œ<code>X.var</code>â€ and â€œ<code>(19/20)*X.var</code>â€.</p>
<p>Let us start by the investigation of the expectation of the estimators. Recall that when we analyzed the sample average as an estimator of the expectation of a measurement we obtained that the expectation of the sampling distribution of the estimator is equal to the value of the parameter it is trying to estimate. One may wonder: What is the situation for the estimators of the variance? Is it or is it not the case that the expectation of their sampling distribution equals the value of the variance? In other words, is the distribution of either estimators of the variance centered at the value of the parameter they are trying to estimate?</p>
<p>Compute the expectations of the two estimators:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(X.var)</code></pre></div>
<pre><code>## [1] 2.9968456</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>((<span class="dv">19</span><span class="op">/</span><span class="dv">20</span>)<span class="op">*</span>X.var)</code></pre></div>
<pre><code>## [1] 2.8470033</code></pre>
<p>Note that 3 is the value of the variance of the measurement that was used in the simulation. Observe that the expectation of <span class="math inline">\(S^2\)</span> is essentially equal to 3, whereas the expectation of the alternative estimator is less than 3. Hence, at least in the example that we consider, the center of the distribution of <span class="math inline">\(S^2\)</span> is located on the target value. On the other hand, the center of the sampling distribution of the alternative estimator is located off that target value.</p>
<p>As a matter of fact it can be shown mathematically that the expectation of the estimator <span class="math inline">\(S^2\)</span> is always equal to the variance of the measurement. This holds true regardless of what is the actual value of the variance. On the other hand the expectation of the alternative estimator is always off the target value<a href="#fn39" class="footnoteRef" id="fnref39"><sup>39</sup></a>.</p>
<p>An estimator is called <em>unbiased</em> if its expectation is equal to the value of the parameter that it tries to estimate. We get that <span class="math inline">\(S^2\)</span> is an unbiased estimator of the variance. Similarly, the sample average is an unbiased estimator of the expectation. Unlike these two estimators, the alternative estimator of the variance is a <em>biased</em> estimator.</p>
<p>The default is to use <span class="math inline">\(S^2\)</span> as the estimator of the variance of the measurement and to use its square root as the estimator of the standard deviation of the measurement. A justification, which is frequently quoted to justify this selection, is the fact that <span class="math inline">\(S^2\)</span> is an unbiased estimator of the variance<a href="#fn40" class="footnoteRef" id="fnref40"><sup>40</sup></a>.</p>
<p>In the previous section, when comparing two competing estimators of the expectation, or main concern was the quantification of the spread of the sampling distribution of either estimator about the target value of the parameter. We used that spread as a measure of the distance between the estimator and the value it tries to estimate. In the setting of the previous section both estimators were unbiased. Consequently, the variance of the estimators, which measures the spread of the distribution about its expectation, could be used in order to quantify the distance between the estimator and the parameter. (Since, for unbiased estimators, the parameter is equal to the expectation of the sampling distribution.)</p>
<p>In the current section one of the estimators (<span class="math inline">\(S^2\)</span>) is unbiased, but the other (the alternative estimator) is not. In order to compare their accuracy in estimation we need to figure out a way to quantify the distance between a biased estimator and the value it tries to estimate.</p>
<p>Towards that end let us recall the definition of the variance. Given a random variable <span class="math inline">\(X\)</span> with an expectation <span class="math inline">\(\Expec(X)\)</span>, we consider the square of the deviations <span class="math inline">\((X - \Expec(X))^2\)</span>, which measure the (squared) distance between each value of the random variable and the expectation. The variance is defined as the expectation of the squared distance: <span class="math inline">\(\Var(X) = \Expec[(X-\Expec(X))^2]\)</span>. One may think of the variance as an overall measure of the distance between the random variable and the expectation.</p>
<p>Assume now that the goal is to assess the distance between an estimator and the parameter it tries to estimate. In order to keep the discussion on an abstract level let us use the Greek letter <span class="math inline">\(\theta\)</span> (read: theta) to denote this parameter<a href="#fn41" class="footnoteRef" id="fnref41"><sup>41</sup></a>. The estimator is denoted by <span class="math inline">\(\hat \theta\)</span> (read: theta hat). It is a statistic, a formula applied to the data. Hence, with respect to the sampling distribution, <span class="math inline">\(\hat \theta\)</span> is a random variable<a href="#fn42" class="footnoteRef" id="fnref42"><sup>42</sup></a>. The issue is to measure the distance between the random variable <span class="math inline">\(\hat \theta\)</span> and the parameter <span class="math inline">\(\theta\)</span>.</p>
<p>Motivated by the method that led to the definition of the variance we consider the deviations between the estimator and the parameter. The square deviations <span class="math inline">\((\hat \theta - \theta)^2\)</span> may be considered in the current context as a measure of the (squared) distance between the estimator and the parameter. When we take the expectation of these square deviations we get an overall measure of the distance between the estimator and the parameter. This overall distance is called the <em>mean square error</em> of the estimator and is denoted by MSE:</p>
<p><span class="math display">\[\mathrm{MSE} = \Expec\big[(\hat \theta - \theta)^2\big]\;.\]</span></p>
<p>The mean square error of an estimator is tightly linked to the bias and the variance of the estimator. The bias of an estimator <span class="math inline">\(\hat \theta\)</span> is the difference between the expectation of the estimator and the parameter it seeks to estimate:</p>
<p><span class="math display">\[\mathrm{Bias} = \Expec(\hat \theta) - \theta\;.\]</span> In an unbiased estimator the expectation of the estimator and the estimated parameter coincide, i.e.Â the bias is equal to zero. For a biased estimator the bias is either negative, as is the case for the alternative estimator of the variance, or else it is positive.</p>
<p>The variance of the estimator, <span class="math inline">\(\mbox{Variance} = \Var(\hat \theta)\)</span>, is a measure of the spread of the sampling distribution of the estimator about its expectation.</p>
<p>The link between the mean square error, the bias, and the variance is described by the formula:</p>
<p><span class="math display">\[\mbox{MSE} = \mbox{Variance} + (\mbox{Bias})^2\;.\]</span> Hence, the mean square error of an estimator is the sum of its variance, the (squared) distance between the estimator and its expectation, and the square of the bias, the square of the distance between the expectation and the parameter. The mean square error is influenced both by the spread of the distribution about the expected value (the variance) and by the distance between the expected value and the parameter (the bias). The larger either of them become the larger is the mean square error, namely the distance between the estimator and the parameter.</p>
<p>Let us compare between the mean square error of the estimator <span class="math inline">\(S^2\)</span> and the mean square error of the alternative estimator <span class="math inline">\([19/20] S^2\)</span>. Recall that we have computed their expectations and found out that the expectation of <span class="math inline">\(S^2\)</span> is essentially equal to 3, the target value of the variance. The expectation of the alternative estimator turned out to be equal to 2.845630, which is less than the target value<a href="#fn43" class="footnoteRef" id="fnref43"><sup>43</sup></a>. It turns out that the bias of <span class="math inline">\(S^2\)</span> is zero (or essentially zero in the simulations) and the bias of the alternative estimator is <span class="math inline">\(2.845630 - 3 = -0.15437 \approx -0.15\)</span>.</p>
<p>In order to compute the mean square errors of both estimators, let us compute their variances:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(X.var)</code></pre></div>
<pre><code>## [1] 0.9503053</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>((<span class="dv">19</span><span class="op">/</span><span class="dv">20</span>)<span class="op">*</span>X.var)</code></pre></div>
<pre><code>## [1] 0.85765053</code></pre>
<p>Observe that the variance of <span class="math inline">\(S^2\)</span> is essentially equal to 0.936 and the variance of the alternative estimator is essentially equal to 0.845.</p>
<p>The estimator <span class="math inline">\(S^2\)</span> is unbiased. Consequently, the mean square error of <span class="math inline">\(S^2\)</span> is equal to its variance. The bias of the alternative is -0.15. As a result we get that the mean square error of this estimator, which is the sum of the variance and the square of the bias, is essentially equal to</p>
<p><span class="math display">\[0.845 + (-0.15)^2 = 0.845 + 0.0225 = 0.8675\;.\]</span> Observe that the mean square error of the estimator <span class="math inline">\(S^2\)</span>, which is equal to 0.936, is larger than the mean square error of the alternative estimator.</p>
<p>Notice that even though the alternative estimator is biased it still has a smaller mean square error than the default estimator <span class="math inline">\(S^2\)</span>. Indeed, it can be prove mathematically that when the measurement has a Normal distribution then the mean square error of the alternative estimator is always smaller than the mean square error of the sample variance <span class="math inline">\(S^2\)</span>.</p>
<p>Still, although the alternative estimator is slightly more accurate than <span class="math inline">\(S^2\)</span> in the estimation of the variance, the tradition is to use the latter. Obeying this tradition we will henceforth use <span class="math inline">\(S^2\)</span> whenever estimation of the variance is required. Likewise, we will use <span class="math inline">\(S\)</span>, the square root of the sample variance, to estimate the standard deviation.</p>
<p>In order to understand how is it that the biased estimator produced a smaller mean square error than the unbiased estimator let us consider the two components of the mean square error. The alternative estimator is biased but, on the other hand, it has a smaller variance. Both the bias and the variance contribute to the mean square error of an estimator. The price for reducing the bias in estimation is usually an increase in the variance and vice versa. The consequence of producing an unbiased estimator such as <span class="math inline">\(S^2\)</span> is an inflated variance. A better estimator is an estimator that balances between the error that results from the bias and the error that results from the variance. Such is the alternative estimator.</p>
<p>We will use <span class="math inline">\(S^2\)</span> in order to estimate the variance of a measurement. A context in which an estimate of the variance of a measurement is relevant is in the assessment of the variance of the sample mean. Recall that the variance of the sample mean is equal to <span class="math inline">\(\Var(X)/n\)</span>, where <span class="math inline">\(\Var(X)\)</span> is the variance of the measurement and <span class="math inline">\(n\)</span> is the size of the sample. In the case where the variance of the measurement is not known one may estimate it from the sample using <span class="math inline">\(S^2\)</span>. It follows that the estimator of the variance of the sample average is <span class="math inline">\(S^2/n\)</span>. Similarly, <span class="math inline">\(S/\sqrt{n}\)</span> can be used as an estimator of the standard deviation of the sample average.</p>
</div>
<div id="EstimationOtherPars" class="section level2">
<h2><span class="header-section-number">10.5</span> Estimation of Other Parameters</h2>
<p>In the previous two section we considered the estimation of the expectation and the variance of a measurement. The proposed estimators, the sample average for the expectation and the sample variance for the variance, are not tied to any specific model for the distribution of the measurement. They may be applied to data whether or not a theoretical model for the distribution of the measurement is assumed.</p>
<p>In the cases where a theoretical model for the measurement is assumed one may be interested in the estimation of the specific parameters associated with this model. In the first part of the book we introduced the Binomial, the Poisson, the Uniform, the Exponential, and the Normal models for the distribution of measurements. In this section we consider the estimation of the parameters that determine each of these theoretical distributions based on a sample generated from the same distribution. In some cases the estimators coincide with the estimators considered in the previous sections. In other cases the estimators are different.</p>
<p>Start with the Binomial distribution. We will be interested in the special case <span class="math inline">\(X \sim \mathrm{Binomial}(1,p)\)</span>. This case involves the outcome of a single trial. The trial has two possible outcomes, one of them is designated as â€œsuccessâ€ and the other as â€œfailureâ€. The parameter <span class="math inline">\(p\)</span> is the probability of the success. The <span class="math inline">\(\mathrm{Binomial}(1,p)\)</span> distribution is also called <em>the Bernoulli distribution</em>. Our concern is the estimation of the parameter <span class="math inline">\(p\)</span> based on a sample of observations from this Bernoulli distribution.</p>
<p>This estimation problem emerges in many settings that involve the assessment of the probability of an event based on a sample of <span class="math inline">\(n\)</span> observations. In each observation the event either occurs or not. A natural estimator of the probability of the event is its relative frequency in the sample. Let us show that this estimator can be represented as an average of a Bernoulli sample and the sample average is used for the estimation of a Bernoulli expectation.</p>
<p>Consider an event, one may code a measurement <span class="math inline">\(X\)</span>, associated with an observation, by 1 if the event occurs and by 0 if it does not. Given a sample of size <span class="math inline">\(n\)</span>, one thereby produces <span class="math inline">\(n\)</span> observations with values 0 or 1. An observation has the value 1 if the event occurs for that observation or, else, the value is 0.</p>
<p>Notice that <span class="math inline">\(\Expec(X) = 1 \cdot p = p\)</span>. Consequently, the probability of the event is equal to the expectation of the Bernoulli measurement<a href="#fn44" class="footnoteRef" id="fnref44"><sup>44</sup></a>. It turns out that the parameter one seeks to estimate is the expectation of a Bernoulli measurement. The estimation is based on a sample of size <span class="math inline">\(n\)</span> of Bernoulli observations.</p>
<p>In SectionÂ <a href="ChapEstimation.html#EstimationExp">10.3</a> it was proposed to use the sample average as an estimate of the expectation. The sample average is the sum of the observations, divided by the number of observation. In the specific case of a sample of Bernoulli observations, the sum of observation is the sum of zeros and one. The zeros do not contribute to the sum. Hence, the sum is equal to the number of times that 1 occurs, namely the frequency of the occurrences of the event. When we divide by the sample size we get the relative frequency of the occurrences. The conclusion is that the sample average of the Bernoulli observations and the relative frequency of occurrences of the event in the sample are the same. Consequently, the sample relative frequency of the event is also a sample average that estimates the expectation of the Bernoulli measurement.</p>
<p>We seek to estimate <span class="math inline">\(p\)</span>, the probability of the event. The estimator is the relative frequency of the event in the sample. We denote this estimator by <span class="math inline">\(\hat P\)</span>. This estimator is a sample average of Bernoulli observations that is used in order to estimate the expectation of the Bernoulli distribution. From the discussion in SectionÂ <a href="ChapEstimation.html#EstimationExp">10.3</a> one may conclude that this estimator is an unbiased estimator of <span class="math inline">\(p\)</span> (namely, <span class="math inline">\(\Expec(\hat P) = p\)</span>) and that its variance is equal to:</p>
<p><span class="math display">\[\Var(\hat P) = \Var(X) / n = p(1-p)/n\;,\]</span> where the variance of the measurement is obtained from the formula for the variance of a <span class="math inline">\(\mathrm{Binomial}(1,p)\)</span> distribution<a href="#fn45" class="footnoteRef" id="fnref45"><sup>45</sup></a>.</p>
<p>The second example of an integer valued random variable that was considered in the first part of the book is the <span class="math inline">\(\mathrm{Poisson}(\lambda)\)</span> distribution. Recall that <span class="math inline">\(\lambda\)</span> is the expectation of a Poisson measurement. Hence, one may use the sample average of Poisson observations in order to estimate this parameter.</p>
<p>The first example of a continuous distribution that was discussed in the first part of the book is the <span class="math inline">\(\mathrm{Uniform}(a,b)\)</span> distribution. This distribution is parameterized by <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, the end-points of the interval over which the distribution is defined. A natural estimator of <span class="math inline">\(a\)</span> is the smallest value observed and a natural estimator of <span class="math inline">\(b\)</span> is the largest value. One may use the function â€œ<code>min</code>â€ for the computation of the former estimate from the sample and use the function â€œ<code>max</code>â€ for the computation of the later. Both estimators are slightly biased but have a relatively small mean square error.</p>
<p>Next considered the <span class="math inline">\(X \sim \mathrm{Exponential}(\lambda)\)</span> random variable. This distribution was applied in this chapter to model the distribution of the prices of cars. The distribution is characterized by the rate parameter <span class="math inline">\(\lambda\)</span>. In order to estimate the rate one may notice the relation between it and the expectation of the measurement:</p>
<p><span class="math display">\[\Expec(X) = 1/\lambda \quad \Longrightarrow \quad \lambda = 1/\Expec(X)\;.\]</span></p>
<p>The rate is equal to the reciprocal of the expectation. The expectation can be estimated by the sample average. Hence a natural proposal is to use the reciprocal of the sample average as an estimator of the rate:</p>
<p><span class="math display">\[\hat \lambda = 1/ \bar X\;.\]</span></p>
<p>The final example that we mention is the <span class="math inline">\(\mathrm{Normal}(\mu,\sigma^2)\)</span> case. The parameter <span class="math inline">\(\mu\)</span> is the expectation of the measurement and may be estimated by the sample average <span class="math inline">\(\bar X\)</span>. The parameter <span class="math inline">\(\sigma^2\)</span> is the variance of a measurement, and can be estimated using the sample variance <span class="math inline">\(S^2\)</span>.</p>
</div>
<div id="exercises-5" class="section level2">
<h2><span class="header-section-number">10.6</span> Exercises</h2>

<div class="exercise">
<p><span id="exr:unnamed-chunk-150" class="exercise"><strong>Exercise 10.1  </strong></span>In SubsectionÂ <a href="ChapEstimation.html#ComparingEstimators">10.3.2</a> we compare the average against the mid-range as estimators of the expectation of the measurement. The goal of this exercise is to repeat the analysis, but this time compare the average to the median as estimators of the expectation in symmetric distributions.</p>
<ol style="list-style-type: decimal">
<li><p>Simulate the sampling distribution of average and the median of a sample of size <span class="math inline">\(n=100\)</span> from the <span class="math inline">\(\mathrm{Normal}(3,2)\)</span> distribution. Compute the expectation and the variance of the sample average and of the sample median. Which of the two estimators has a smaller mean square error?</p></li>
<li>Simulate the sampling distribution of average and the median of a sample of size <span class="math inline">\(n=100\)</span> from the <span class="math inline">\(\mathrm{Uniform}(0.5,5.5)\)</span> distribution. Compute the expectation and the variance of the sample average and of the sample median. Which of the two estimators has a smaller mean square error?</li>
</ol>
</div>


<div class="exercise">
<p><span id="exr:unnamed-chunk-151" class="exercise"><strong>Exercise 10.2  </strong></span>The goal in this exercise is to assess estimation of a proportion in a population on the basis of the proportion in the sample.</p>
<p>The file â€œ<code>pop2.csv</code>â€ was introduced in ExerciseÂ <a href="ChapSampDist.html#exr:ex1sampdist">7.1</a> of ChapterÂ <a href="ChapSampDist.html#ChapSampDist">7</a>. This file contains information associated to the blood pressure of an imaginary population of size 100,000. The file can be found on the internet (<a href="http://pluto.huji.ac.il/~msby/StatThink/Datasets/pop2.csv" class="uri">http://pluto.huji.ac.il/~msby/StatThink/Datasets/pop2.csv</a>). One of the variables in the file is a factor by the name â€œ<code>group</code>â€ that identifies levels of blood pressure. The levels of this variable are â€œ<code>HIGH</code>â€, â€œ<code>LOW</code>â€, and â€œ<code>NORMAL</code>â€.</p>
<p>The file â€œ<code>ex2.csv</code>â€ contains a sample of size <span class="math inline">\(n=150\)</span> taken from the given population. This file can also be found on the internet (<a href="http://pluto.huji.ac.il/~msby/StatThink/Datasets/ex2.csv" class="uri">http://pluto.huji.ac.il/~msby/StatThink/Datasets/ex2.csv</a>). It contains the same variables as in the file â€œ<code>pop2.csv</code>â€. The file â€œ<code>ex2.csv</code>â€ corresponds in this exercise to the observed sample and the file â€œ<code>pop2.csv</code>â€ corresponds to the unobserved population.</p>
<p>Download both files to your computer and answer the following questions:</p>
<ol style="list-style-type: decimal">
<li><p>Compute the proportion in the sample of those with a high level of blood pressure<a href="#fn46" class="footnoteRef" id="fnref46"><sup>46</sup></a>.</p></li>
<li><p>Compute the proportion in the population of those with a high level of blood pressure.</p></li>
<li><p>Simulate the sampling distribution of the sample proportion and compute its expectation.</p></li>
<li><p>Compute the variance of the sample proportion.</p></li>
<li>It is proposed in SectionÂ <a href="ChapEstimation.html#EstimationOtherPars">10.5</a> that the variance of the sample proportion is <span class="math inline">\(\Var(\hat P) = p(1-p)/n\)</span>, where <span class="math inline">\(p\)</span> is the probability of the event (having a high blood pressure in our case) and <span class="math inline">\(n\)</span> is the sample size (<span class="math inline">\(n=150\)</span> in our case). Examine this proposal in the current setting.</li>
</ol>
</div>

</div>
<div id="summary-8" class="section level2">
<h2><span class="header-section-number">10.7</span> Summary</h2>
<div id="glossary" class="section level3 unnumbered">
<h3>Glossary</h3>
<dl>
<dt>Point Estimation:</dt>
<dd><p>An attempt to obtain the best guess of the value of a population parameter. An estimator is a statistic that produces such a guess. The estimate is the observed value of the estimator.</p>
</dd>
<dt>Bias:</dt>
<dd><p>The difference between the expectation of the estimator and the value of the parameter. An estimator is unbiased if the bias is equal to zero. Otherwise, it is biased.</p>
</dd>
<dt>Mean Square Error (MSE):</dt>
<dd><p>A measure of the concentration of the distribution of the estimator about the value of the parameter. The mean square error of an estimator is equal to the sum of the variance and the square of the bias. If the estimator is unbiased then the mean square error is equal to the variance.</p>
</dd>
<dt>Bernoulli Random Variable:</dt>
<dd><p>A random variable that obtains the value â€œ1â€ with probability <span class="math inline">\(p\)</span> and the value â€œ0â€ with probability <span class="math inline">\(1-p\)</span>. It coincides with the <span class="math inline">\(\mathrm{Binomial}(1,p)\)</span> distribution. Frequently, the Bernoulli random variable emerges as the indicator of the occurrence of an event.</p>
</dd>
</dl>
</div>
<div id="discuss-in-the-forum" class="section level3 unnumbered">
<h3>Discuss in the forum</h3>
<p>Performance of estimators is assessed in the context of a theoretical model for the sampling distribution of the observations. Given a criteria for optimality, an optimal estimator is an estimator that performs better than any other estimator with respect to that criteria. A robust estimator, on the other hand, is an estimator that is not sensitive to misspecification of the theoretical model. Hence, a robust estimator may be somewhat inferior to an optimal estimator in the context of an assumed model. However, if in actuality the assumed model is not a good description of reality then robust estimator will tend to perform better than the estimator denoted optimal.</p>
<p>Some say that optimal estimators should be preferred while other advocate the use of more robust estimators. What is your opinion?</p>
<p>When you formulate your answer to this question it may be useful to come up with an example from you own field of interest. Think of an estimation problem and possible estimators that can be used in the context of this problem. Try to identify a model that is natural to this problem an ask yourself in what ways may this model err in its attempt to describe the real situation in the estimation problem.</p>
<p>As an example consider estimation of the expectation of a Uniform measurement. We demonstrated that the mid-range estimator is better than the sample average if indeed the measurements emerge from the Uniform distribution. However, if the modeling assumption is wrong then this may no longer be the case. If the distribution of the measurement in actuality is not symmetric or if the distribution is more concentrated in the center than in the tails then the performance of the mid-range estimator may deteriorate. The sample average, on the other hand is not sensitive to the distribution not being symmetric.</p>
</div>
<div id="formulas" class="section level3 unnumbered">
<h3>Formulas:</h3>
<ul>
<li><p>Bias: <span class="math inline">\(\mathrm{Bias} = \Expec(\hat \theta) - \theta\)</span>.</p></li>
<li><p>Variance: <span class="math inline">\(\Var(\hat \theta) = \Expec\big[(\hat \theta - \Expec(\hat\theta))^2\big]\)</span>.</p></li>
<li><p>Mean Square Error: <span class="math inline">\(\mathrm{MSE} = \Expec\big[(\hat \theta - \theta)^2\big]\)</span>.</p></li>
</ul>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="34">
<li id="fn34"><p>The name of the argument stands for â€œNA removeâ€. If the value of the argument is set to â€œ<code>TRUE</code>â€ then the missing values are removed in the computation of the average. Consequently, the average is computed for the sub-sequence of non-missing values. The default specification of the argument in the definition of the function is â€œ<code>na.rm=FALSE</code>â€, which implies a missing value for the mean when computed on a sequence that contains missing values.<a href="ChapEstimation.html#fnref34">â†©</a></p></li>
<li id="fn35"><p>As a matter of fact, the difference is the probability of falling in the half-open interval <span class="math inline">\((12000,14000]\)</span>. However, for continuous distributions the probability of the end-points is zero and they do not contribute to the probability of the interval.<a href="ChapEstimation.html#fnref35">â†©</a></p></li>
<li id="fn36"><p>As a matter of fact, the variance of the sample average is exactly <span class="math inline">\(\Var(X)/100 = 0.02\)</span>. Due to the inaccuracy of the simulation we got a slightly different variance.<a href="ChapEstimation.html#fnref36">â†©</a></p></li>
<li id="fn37"><p>Observe that the middle range of the <span class="math inline">\(\mathrm{Uniform}(a,b)\)</span> distribution, the middle point between the maximum value of the distribution <span class="math inline">\(b\)</span> and the minimal value <span class="math inline">\(a\)</span>, is <span class="math inline">\((a+b)/2\)</span>, which is equal to the expectation of the distribution<a href="ChapEstimation.html#fnref37">â†©</a></p></li>
<li id="fn38"><p>Actually, the exact value of the variance of the sample average is <span class="math inline">\(\Var(X)/100 = 0.02083333\)</span>. The results of the simulation are consistent with this theoretical computation.<a href="ChapEstimation.html#fnref38">â†©</a></p></li>
<li id="fn39"><p>For the estimator <span class="math inline">\(S^2\)</span> we get that <span class="math inline">\(\Expec(S^2) = \Var(X)\)</span>. On the other hand, for the alternative estimator we get that <span class="math inline">\(\Expec([(n-1)/n]\cdot S^2) = [(n-1)/n]\Var(X) \not = \Var(X)\)</span>. This statement holds true also in the cases where the distribution of the measurement is not Normal.<a href="ChapEstimation.html#fnref39">â†©</a></p></li>
<li id="fn40"><p>As part of your homework assignment you are required to investigate the properties of <span class="math inline">\(S\)</span>, the square root of <span class="math inline">\(S^2\)</span>, as an estimator of the standard deviation of the measurement. A conclusion of this investigation is that <span class="math inline">\(S\)</span> is a biased estimator of the standard deviation.<a href="ChapEstimation.html#fnref40">â†©</a></p></li>
<li id="fn41"><p>The letter <span class="math inline">\(\theta\)</span> is frequently used in the statistical literature to denote a parameter of the distribution. In the previous section we considered <span class="math inline">\(\theta = \Expec(X)\)</span> and in this section we consider <span class="math inline">\(\theta=\Var(X)\)</span>.<a href="ChapEstimation.html#fnref41">â†©</a></p></li>
<li id="fn42"><p>Observe that we diverge here slightly from our promise to use capital letters to denote random variables. However, denoting the parameter by <span class="math inline">\(\theta\)</span> and denoting the estimator of the parameter by <span class="math inline">\(\hat \theta\)</span> is standard in the statistical literature. As a matter of fact, we will use the â€œhatâ€ notation, where a hat is placed over a Greek letter that represents the parameter, in other places in this book. The letter with the hat on top will represent the estimator and will always be considered as a random variable. For Latin letters we will still use capital letters, with or without a hat, to represent a random variable and small letter to represent evaluation of the random variable for given data.<a href="ChapEstimation.html#fnref42">â†©</a></p></li>
<li id="fn43"><p>It can be shown mathematically that <span class="math inline">\(\Expec([(n-1)/n] S^2) = [(n-1)/n] \Expec(S^2)\)</span>. Consequently, the actual value of the expectation of the alternative estimator in the current setting is <span class="math inline">\([19/20]\cdot 3 = 2.85\)</span> and the bias is <span class="math inline">\(-0.15\)</span>. The results of the simulation are consistent with this fact.<a href="ChapEstimation.html#fnref43">â†©</a></p></li>
<li id="fn44"><p>The expectation of <span class="math inline">\(X\sim\mathrm{Binomial}(n,p)\)</span> is <span class="math inline">\(\Expec(X)=np\)</span>. In the Bernoulli case <span class="math inline">\(n=1\)</span>. Therefore, <span class="math inline">\(\Expec(X) = 1\cdot p = p\)</span>.<a href="ChapEstimation.html#fnref44">â†©</a></p></li>
<li id="fn45"><p>The variance of <span class="math inline">\(X\sim\mathrm{Binomial}(n,p)\)</span> is <span class="math inline">\(\Var(X)=np(1-p)\)</span>. In the Bernoulli case <span class="math inline">\(n=1\)</span>. Therefore, <span class="math inline">\(\Var(X) = 1\cdot p(1-p) = p(1-p)\)</span>.<a href="ChapEstimation.html#fnref45">â†©</a></p></li>
<li id="fn46"><p>Hint: You may use the function summary or you may note that the expression â€œ<em>variable</em><code>==</code><em>level</em>â€ produces a sequence with logical â€œ<code>TRUE</code>â€ or â€œ<code>FALSE</code>â€ entries that identify entries in the sequence â€œ<em>variable</em>â€ that obtain the value â€œ<em>level</em>â€.<a href="ChapEstimation.html#fnref46">â†©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ChapInference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ChapConfidence.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"download": ["statthink.pdf", "thinkstat.epub", "stathink_datasets.zip"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
